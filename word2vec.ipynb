{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts, get_tmpfile\n",
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('/data/Data/yntn/data_labeled.csv')\n",
    "sentences = list(data[\"text\"] )\n",
    "corpus = []\n",
    "for sentence in sentences:\n",
    "    corpus.append([word.lower() for word in str(sentence).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = setting['n']\n",
    "        self.learning_rate = setting['learning_rate']\n",
    "        self.epochs = setting['epochs']\n",
    "        self.window_size = setting['window_size']\n",
    "        \n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [ 0 for i in range(0, self.v_count)] # set all 0\n",
    "        word_vec[self.word_index[word]] = 1 # set one hot\n",
    "        return word_vec\n",
    "    \n",
    "    def gen_training_data(self, setting, corpus):\n",
    "        word_count = defaultdict(int) # defaultdict will have a default value if that key has not been set yet\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        self.v_count = len(word_count.keys()) # vocab size\n",
    "\n",
    "        self.words_list = list(word_count.keys()) # vocab\n",
    "        \n",
    "        self.word_index = dict((word, i) for i , word in enumerate(self.words_list)) #indexing for word in vocab\n",
    "        self.index_word = dict((i, word) for i , word in enumerate(self.words_list))\n",
    "        \n",
    "        traning_data = [] # list of pair [onehot vector of word, list onehot vector of neighbor\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            sentence_len = len(sentence)\n",
    "            \n",
    "            for i, word in enumerate(sentence):\n",
    "                w_target = self.word2onehot(sentence[i]) # one hot vector\n",
    "                w_context = [] # list of one hot vector of neighbors\n",
    "                \n",
    "                for j in range(i - self.window_size, i + self.window_size + 1): #constraint: in window size\n",
    "                    if ((j != i) and (j <= sentence_len - 1) and (j >= 0)): #constraint: in sentence\n",
    "                        w_context.append(self.word2onehot(sentence[j])) \n",
    "                traning_data.append([w_target, w_context])\n",
    "                \n",
    "        return np.array(traning_data)    \n",
    "    \n",
    "    def forward_pass(self, w_t):\n",
    "        h = np.dot(self.weight1.T, w_t)\n",
    "        u = np.dot(self.weight2.T, h)\n",
    "        tmp = np.exp(u - max(u))\n",
    "        y_c = (tmp/tmp.sum(axis = 0)) # softmax\n",
    "        return y_c, h, u\n",
    "    \n",
    "    def backprop(self, err, h, w_target):\n",
    "        delta_weight2 = np.outer(h, err)\n",
    "        delta_weight1 = np.outer(w_target,np.dot(self.weight2, err.T))\n",
    "        print (err.T)\n",
    "        self.weight1 = self.weight1 - (self.learning_rate * delta_weight1) #update\n",
    "        self.weight2 = self.weight2 - (self.learning_rate * delta_weight2) #update\n",
    "   \n",
    "    def train(self, traning_data):\n",
    "        \n",
    "        self.weight1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "        self.weight2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "            \n",
    "        for i in range (self.epochs):\n",
    "            self.loss = 0\n",
    "            for w_target, w_context in traning_data:\n",
    "                y_predict, h, u = self.forward_pass(w_target) #vector for prediction y_predict, hidden layer h and out  put layer u\n",
    "                \n",
    "                err = np.sum([(y_predict - word) for word in w_context], axis = 0) # sum up the difference between y_pred and each of the context words in w_c\n",
    "       \n",
    "                self.backprop(err, h, w_target)\n",
    "        \n",
    "                self.loss += - np.sum([u[word.index(1)] for word in w_context]) + len(w_context) * np.log(np.sum(np.exp(u)))\n",
    "                \n",
    "            print(\"Epoch: \" , i, \"Loss: \", self.loss)  \n",
    "         \n",
    "    def wordtovec(self, word):\n",
    "#         w_index = self.word_index[word]\n",
    "#         v_w = self.w1[w_index]\n",
    "       return self.weight1[self.word_index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01,\n",
       "       1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03,\n",
       "       8.10308393e+03, 2.98095799e+03, 1.09663316e+03, 4.03428793e+02,\n",
       "       1.48413159e+02, 5.45981500e+01, 2.00855369e+01, 7.38905610e+00,\n",
       "       2.71828183e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.array([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1])\n",
    "np.exp(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.55051013e-04, 4.21472352e-04, 1.14568064e-03, 3.11428285e-03,\n",
       "       8.46549849e-03, 2.30116107e-02, 6.25520432e-02, 1.70034082e-01,\n",
       "       4.62200557e-01, 1.70034082e-01, 6.25520432e-02, 2.30116107e-02,\n",
       "       8.46549849e-03, 3.11428285e-03, 1.14568064e-03, 4.21472352e-04,\n",
       "       1.55051013e-04])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.array([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1])\n",
    "tmp = np.exp(vec - max(vec))\n",
    "y_c = (tmp/tmp.sum(axis = 0))\n",
    "# tmp\n",
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = {'n': 10, 'learning_rate': 0.01, 'epochs':50, 'window_size': 3}\n",
    "# corpus = [ ['trog', 'ảnh', 'thế', 'thôi', 'chứ', 'ngoài', 'còn', 'bé', 'mà']]\n",
    "w2v = word2vec()\n",
    "\n",
    "training_data = w2v.gen_training_data(setting, corpus)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
