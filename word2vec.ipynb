{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.test.utils import common_texts, get_tmpfile\n",
    "# from gensim.models import Word2Vec, KeyedVectors\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "data = pd.read_csv('/data/Data/yntn/data_labeled.csv')\n",
    "sentences = list(data[\"text\"] )\n",
    "corpus = []\n",
    "for sentence in sentences:\n",
    "    corpus.append([word.lower() for word in str(sentence).split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = defaultdict(int) # defaultdict will have a default value if that key has not been set yet\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence:\n",
    "        word_count[word] += 1\n",
    "v_count = len(word_count.keys()) # vocab size\n",
    "# v_count = 207175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = corpus[:1000] # take 10000 sentences for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word2vec():\n",
    "    def __init__(self):\n",
    "        self.n = setting['n']\n",
    "        self.learning_rate = setting['learning_rate']\n",
    "        self.epochs = setting['epochs']\n",
    "        self.window_size = setting['window_size']\n",
    "        \n",
    "    def word2onehot(self, word):\n",
    "        word_vec = [ 0 for i in range(0, self.v_count)] # set all 0\n",
    "        word_vec[self.word_index[word]] = 1 # set one hot\n",
    "        return word_vec\n",
    "    \n",
    "    def gen_training_data(self, setting, corpus):\n",
    "        word_count = defaultdict(int) # defaultdict will have a default value if that key has not been set yet\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            for word in sentence:\n",
    "                word_count[word] += 1\n",
    "        \n",
    "        self.v_count = len(word_count.keys()) # vocab size\n",
    "\n",
    "        self.words_list = list(word_count.keys()) # vocab\n",
    "        \n",
    "        self.word_index = dict((word, i) for i , word in enumerate(self.words_list)) #indexing for word in vocab\n",
    "        self.index_word = dict((i, word) for i , word in enumerate(self.words_list))\n",
    "        \n",
    "        traning_data = [] # list of pair [onehot vector of word, list onehot vector of neighbor\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            sentence_len = len(sentence)\n",
    "            \n",
    "            for i, word in enumerate(sentence):\n",
    "                w_target = self.word2onehot(sentence[i]) # one hot vector\n",
    "                w_context = [] # list of one hot vector of neighbors\n",
    "                \n",
    "                for j in range(i - self.window_size, i + self.window_size + 1): #constraint: in window size\n",
    "                    if ((j != i) and (j <= sentence_len - 1) and (j >= 0)): #constraint: in sentence\n",
    "                        w_context.append(self.word2onehot(sentence[j])) \n",
    "                if len(w_context) != 0:\n",
    "                    traning_data.append([w_target, w_context])\n",
    "                \n",
    "        return np.array(traning_data)    \n",
    "    \n",
    "    def forward_pass(self, w_t):\n",
    "        h = np.dot(self.weight1.T, w_t)\n",
    "        u = np.dot(self.weight2.T, h)\n",
    "        tmp = np.exp(u - max(u))\n",
    "        y_c = (tmp/tmp.sum(axis = 0)) # softmax\n",
    "        return y_c, h, u\n",
    "    \n",
    "    def backprop(self, err, h, w_target):\n",
    "        delta_weight2 = np.outer(h, err)\n",
    "        delta_weight1 = np.outer(w_target, np.dot(self.weight2, err.T))\n",
    "#         print (err.T)\n",
    "        self.weight1 = self.weight1 - (self.learning_rate * delta_weight1) #update delta 1\n",
    "        self.weight2 = self.weight2 - (self.learning_rate * delta_weight2) #update delta 2\n",
    "   \n",
    "    def train(self, traning_data):\n",
    "        \n",
    "        self.weight1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "        self.weight2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "            \n",
    "        for i in range (self.epochs):\n",
    "            self.loss = 0\n",
    "            for w_target, w_context in traning_data:\n",
    "                y_predict, h, u = self.forward_pass(w_target) #vector for prediction y_predict, hidden layer h and out  put layer u\n",
    "                \n",
    "                err = np.sum([(y_predict - word) for word in w_context], axis = 0) # sum up the difference between y_pred and each of the context words in w_c\n",
    "       \n",
    "                self.backprop(err, h, w_target)\n",
    "        \n",
    "                self.loss += - np.sum([u[word.index(1)] for word in w_context]) + len(w_context) * np.log(np.sum(np.exp(u)))\n",
    "                \n",
    "            print(\"Epoch: \" , i, \"Loss: \", self.loss)  \n",
    "         \n",
    "    def wordtovec(self, word):\n",
    "#         w_index = self.word_index[word]\n",
    "#         v_w = self.weight1[w_index]\n",
    "       return self.weight1[self.word_index[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.71828183e+00, 7.38905610e+00, 2.00855369e+01, 5.45981500e+01,\n",
       "       1.48413159e+02, 4.03428793e+02, 1.09663316e+03, 2.98095799e+03,\n",
       "       8.10308393e+03, 2.98095799e+03, 1.09663316e+03, 4.03428793e+02,\n",
       "       1.48413159e+02, 5.45981500e+01, 2.00855369e+01, 7.38905610e+00,\n",
       "       2.71828183e+00])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.array([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1])\n",
    "np.exp(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.55051013e-04, 4.21472352e-04, 1.14568064e-03, 3.11428285e-03,\n",
       "       8.46549849e-03, 2.30116107e-02, 6.25520432e-02, 1.70034082e-01,\n",
       "       4.62200557e-01, 1.70034082e-01, 6.25520432e-02, 2.30116107e-02,\n",
       "       8.46549849e-03, 3.11428285e-03, 1.14568064e-03, 4.21472352e-04,\n",
       "       1.55051013e-04])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = np.array([1,2,3,4,5,6,7,8,9,8,7,6,5,4,3,2,1])\n",
    "tmp = np.exp(vec - max(vec))\n",
    "y_c = (tmp/tmp.sum(axis = 0))\n",
    "# tmp\n",
    "y_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  243839.0395701932\n",
      "Epoch:  1 Loss:  238210.05772951845\n",
      "Epoch:  2 Loss:  234109.6099370733\n",
      "Epoch:  3 Loss:  229466.35203308385\n",
      "Epoch:  4 Loss:  224381.7489361988\n",
      "Epoch:  5 Loss:  219671.84433243552\n",
      "Epoch:  6 Loss:  215679.9199989828\n",
      "Epoch:  7 Loss:  212390.53456370012\n",
      "Epoch:  8 Loss:  209632.85779615273\n",
      "Epoch:  9 Loss:  207272.54095817383\n",
      "Epoch:  10 Loss:  205209.88845309132\n",
      "Epoch:  11 Loss:  203375.94075450898\n",
      "Epoch:  12 Loss:  201725.11964217472\n",
      "Epoch:  13 Loss:  200226.21241248868\n",
      "Epoch:  14 Loss:  198853.24571450712\n",
      "Epoch:  15 Loss:  197581.93173511763\n",
      "Epoch:  16 Loss:  196391.6509680202\n",
      "Epoch:  17 Loss:  195266.51222460627\n",
      "Epoch:  18 Loss:  194193.94559926\n",
      "Epoch:  19 Loss:  193163.47324891403\n",
      "Epoch:  20 Loss:  192166.95897473136\n",
      "Epoch:  21 Loss:  191198.48144263044\n",
      "Epoch:  22 Loss:  190253.69251674757\n",
      "Epoch:  23 Loss:  189329.32731147917\n",
      "Epoch:  24 Loss:  188422.90969611084\n",
      "Epoch:  25 Loss:  187532.568602504\n",
      "Epoch:  26 Loss:  186656.93182780093\n",
      "Epoch:  27 Loss:  185795.069620771\n",
      "Epoch:  28 Loss:  184946.44011470952\n",
      "Epoch:  29 Loss:  184110.80649366102\n",
      "Epoch:  30 Loss:  183288.13508266088\n",
      "Epoch:  31 Loss:  182478.50051547043\n",
      "Epoch:  32 Loss:  181682.01837056642\n",
      "Epoch:  33 Loss:  180898.81080973224\n",
      "Epoch:  34 Loss:  180128.99681411564\n",
      "Epoch:  35 Loss:  179372.69292317415\n",
      "Epoch:  36 Loss:  178630.01454375059\n",
      "Epoch:  37 Loss:  177901.07535444418\n",
      "Epoch:  38 Loss:  177185.9854858731\n",
      "Epoch:  39 Loss:  176484.84883985363\n",
      "Epoch:  40 Loss:  175797.76014210697\n",
      "Epoch:  41 Loss:  175124.80268318494\n",
      "Epoch:  42 Loss:  174466.04671049374\n",
      "Epoch:  43 Loss:  173821.547375008\n",
      "Epoch:  44 Loss:  173191.34150210084\n",
      "Epoch:  45 Loss:  172575.4436375799\n",
      "Epoch:  46 Loss:  171973.84250116194\n",
      "Epoch:  47 Loss:  171386.49875570153\n",
      "Epoch:  48 Loss:  170813.34427979603\n",
      "Epoch:  49 Loss:  170254.2825273258\n"
     ]
    }
   ],
   "source": [
    "setting = {'n': 10, 'learning_rate': 0.01, 'epochs':50, 'window_size': 3}\n",
    "# corpus = [ ['trog', 'ảnh', 'thế', 'thôi', 'chứ', 'ngoài', 'còn', 'bé', 'mà'],['trog', 'ảnh', 'thế', 'thôi', 'chứ', 'ngoài', 'còn', 'bé', 'mà'],['trog', 'ảnh', 'thế', 'thôi', 'chứ', 'ngoài', 'còn', 'bé', 'mà'],\n",
    "#          ['trog', 'ảnh', 'thế', 'thôi', 'chứ'],\n",
    "#          ['thôi', 'chứ', 'ngoài', 'còn']]\n",
    "\n",
    "w2v = word2vec()\n",
    "\n",
    "# cp = [['mot','hai'],['ba']]\n",
    "\n",
    "training_data = w2v.gen_training_data(setting, cp)\n",
    "\n",
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file_Name = \"dict_word2vec.pkl\"\n",
    "fileObject = open(file_Name,'wb') \n",
    "\n",
    "pickle.dump(w2v,fileObject)   \n",
    "\n",
    "fileObject.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
